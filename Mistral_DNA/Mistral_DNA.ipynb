{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d293023",
   "metadata": {},
   "source": [
    "## Training and Inference on the Mistral DNA Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596caa9d",
   "metadata": {},
   "source": [
    "<p><strong>Generative Artificial Intelligence</strong> (AI) represents a cutting-edge domain within machine learning, focused on creating new, synthetic yet realistic data. This includes generating text, images, music, and even biological sequences. At the heart of many generative AI applications are <strong>Large Language Models</strong> (LLMs), which have revolutionized natural language processing and beyond.</p>\n",
    "<p>LLMs are <strong>sophisticated neural networks</strong> trained on vast amounts of text data to understand, generate, and interact with human language. Their architecture, often based on <strong>Transformers</strong>, allows them to capture complex patterns and context within data, making them powerful tools for various applications, from chatbots to creative writing and scientific discovery.</p>\n",
    "<blockquote class=\"details\" style=\"border: 2px solid #ddd; margin: 1em 0.2em\">\n",
    "<div class=\"box-title details-title\" id=\"details-transformers\"><button class=\"gtn-boxify-button details\" type=\"button\" aria-controls=\"details-transformers\" aria-expanded=\"true\"><i class=\"fas fa-info-circle\" aria-hidden=\"true\" ></i> <span>Details:  Transformers </span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>Transformers are a type of neural network model designed to handle sequential data, such as text, by using self-attention mechanisms to weigh the importance of input elements relative to each other, enabling the model to understand and generate coherent and contextually relevant outputs.</p>\n",
    "</blockquote>\n",
    "<p><a href=\"https://mistral.ai/\">Mistral AI</a>, French artificial intelligence (AI) startup, recently launched large language models (LLMs) showing performances superior to Llama2. In particular, Mixtral-8x7B implements:</p>\n",
    "<ul>\n",
    "<li><strong>Grouped-Query Attention</strong>: Efficiently computes attention by grouping queries, reducing computational load and memory usage.</li>\n",
    "<li><strong>Sliding-Window Attention</strong>: Focuses on a fixed-size window of tokens, sliding over the sequence to manage long texts efficiently.</li>\n",
    "<li><strong>Byte-fallback BPE Tokenizer</strong>: Tokenizes text into subword units, falling back to byte-level tokenization for unknown words, ensuring robust handling of diverse text inputs.</li>\n",
    "</ul>\n",
    "<p>These techniques collectively enhance the performance and efficiency of large language models, enabling them to process and generate text more effectively.</p>\n",
    "<p>In this tutorial, we will use a simplified Mistral model architecture with fewer layers and hidden units to reduce computational requirements. The model will be trained to predict the next base in the sequence. For instance, for a sequence like <code style=\"color: inherit\">ATTTGTTGGT</code>, the model will be trained to predict the suffix <code style=\"color: inherit\">TTGGT</code> given the prefix <code style=\"color: inherit\">ATTTG</code>. This process is called <strong>causal language modeling</strong>.</p>\n",
    "<p>To pretrain the model, we will use a file containing 100,000 non-overlapping DNA sequences of 200 bases, corresponding to around 1% of the human genome (hg38 assembly). This involves training the model to predict the end of a DNA sequence.</p>\n",
    "<p>By the end of this tutorial, we will obtain a Mistral-DNA model with an internal representation of DNA sequence grammar. This pretrained model can then be used for various applications, such as fine-tuning for classification tasks or predicting mutational effects.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21458f",
   "metadata": {},
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What are the required dependencies doing?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ul>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">accelerate</code>: A library by <a href=\"https://huggingface.co/\">Hugging Face</a> ‚Äì a platform that provides tools and resources for building, training, and deploying machine learning models ‚Äì designed to simplify the process of training and deploying machine learning models across different hardware environments. It provides tools to optimize performance on GPUs, TPUs, and other accelerators, making it easier to scale models efficiently.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">datasets</code>: A library by Hugging Face for managing and processing datasets. It provides tools to load, manipulate, and share datasets in a standardized format, making it easier to work with machine learning data.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">numpy</code>: A fundamental package for scientific computing in Python.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">torch</code>: Also known as PyTorch, it is an open-source machine learning library developed by Facebook‚Äôs AI Research lab. It provides a flexible platform for building and training neural networks, with a focus on tensor computations and automatic differentiation.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">transformers</code>: A library by Hugging Face that provides implementations of state-of-the-art transformer models for natural language processing (NLP). It includes pre-trained models and tools for fine-tuning, making it easier to apply transformers to various NLP tasks.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">flash-attn</code>: Implementation of FlashAttention, a Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
    "These libraries are widely used in the machine learning and data science communities for their efficiency, flexibility, and extensive functionality.</p>\n",
    "</li>\n",
    "</ul>\n",
    "</details>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790cb1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import accelerate\n",
    "# import flash_attn\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig, # load the configuration of pre-trained model. architecture and hyperparameter of the model\n",
    "    AutoModelForCausalLM, # loads the pretrained causal language model for task like text generation\n",
    "    AutoTokenizer, # load the tokenizer with a pre-trained model. convert the text to tokens\n",
    "    DataCollatorForLanguageModeling, # designed for language modelling task. prepares batches for training by handling padding and masking\n",
    "    EarlyStoppingCallback,  # is used to stop the training, if in the validation performance stops improving to save time and resources\n",
    "    Trainer, # A high level API for training and evaluating the transformers. \n",
    "    TrainingArguments, # define the hyperparameter like learning rate, batch size, epoch, weight decay. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7f7c6",
   "metadata": {},
   "source": [
    "Let‚Äôs look at the original archicture of Mixtral-8x7B-v0.1 which is stored in the data/models/Mixtral-8x7B-v0.1 folder Github https://github.com/raphaelmourad/Mistral-DNA/tree/main/data/models/Mixtral-8x7B-v0.1 \n",
    "\n",
    "\n",
    "<p> In this 8x7B is the 8 is the number of experts and 7B is the 7 Billion parameters </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88d80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/mnt/data/projects/.immune/Personal/DNA-Language-Model/Mistral_DNA/\"\n",
    "os.chdir(savedir)\n",
    "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caff8b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralConfig {\n",
       "  \"_name_or_path\": \"data/models/Mixtral-8x7B-v0.1\",\n",
       "  \"architectures\": [\n",
       "    \"MixtralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"mixtral\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_experts_per_tok\": 1,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"num_local_experts\": 64,\n",
       "  \"output_router_logits\": false,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"router_aux_loss_coef\": 0.02,\n",
       "  \"router_jitter_noise\": 0.0,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.41.1\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4096\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01203ea4",
   "metadata": {},
   "source": [
    "<p>By loading the configuration, we can inspect or modify the model‚Äôs architecture without loading the actual model weights. Let‚Äôs now initialize a causal language model from the loaded configuration object, with a specific attention implementation:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e729665",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_config(config, attn_implementation=\"eager\")\n",
    "# eager specifies the attention implementatin to use. Attention mechanism will be executed \n",
    "# eagerly which can be useful for debugging or when working with dynamic computation graphs\n",
    "# Eager execution runs operations immediatedy as they are called in Python rather than adding \n",
    "# them to graph execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b34038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0419f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameter 105.0 million\n"
     ]
    }
   ],
   "source": [
    "Total_parameters = sum(p.numel() for p in model.parameters()) / 1000 ** 2\n",
    "print(f\"Total Parameter {Total_parameters:.1f} million\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce82b9b",
   "metadata": {},
   "source": [
    "<blockquote class=\"details\" style=\"border: 2px solid #ddd; margin: 1em 0.2em\">\n",
    "<div class=\"box-title details-title\" id=\"details-loaded-functions-and-classes-from-datasets-and-transformers-libraries\"><button class=\"gtn-boxify-button details\" type=\"button\" aria-controls=\"details-loaded-functions-and-classes-from-datasets-and-transformers-libraries\" aria-expanded=\"true\"><i class=\"fas fa-info-circle\" aria-hidden=\"true\" ></i> <span>Details: Loaded functions and classes from datasets and transformers libraries</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">datasets</code>:\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">load_dataset</code>: function to load datasets from the Hugging Face Hub or local files.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code style=\"color: inherit\">transformers</code>:\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">AutoConfig</code>: Automatically loads the configuration for a pre-trained model. It defines the architecture and hyperparameters of the model.</li>\n",
    "<li><code style=\"color: inherit\">AutoModelForCausalLM</code>: Loads a pre-trained causal language model for tasks like text generation, where the model predicts the next token in a sequence.</li>\n",
    "<li><code style=\"color: inherit\">AutoTokenizer</code>: Loads the tokenizer associated with a pre-trained model. It converts text into tokens that the model can process.</li>\n",
    "<li><code style=\"color: inherit\">DataCollatorForLanguageModeling</code>: A data collator specifically designed for language modeling tasks. It prepares batches of data for training by handling padding and masking.</li>\n",
    "<li><code style=\"color: inherit\">EarlyStoppingCallback</code>: A callback used during training to stop the process early if the model‚Äôs performance on the validation set stops &gt; improving, saving time and resources.</li>\n",
    "<li><code style=\"color: inherit\">Trainer</code>: A high-level API for training and evaluating transformer &gt; models. It simplifies the training loop and handles tasks like gradient accumulation and evaluation.</li>\n",
    "<li><code style=\"color: inherit\">TrainingArguments</code>: A class to define the training configuration, including hyperparameters like learning rate, batch size, and number &gt; of epochs. It is used to configure the <code style=\"color: inherit\">Trainer</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p>These components work together to streamline the process of training and fine-tuning transformer models for various NLP tasks.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83abd73a",
   "metadata": {},
   "source": [
    "<p>As expected, the model is a <code style=\"color: inherit\">MixtralForCausalLM</code> model with several key components:</p>\n",
    "<ol>\n",
    "<li>\n",
    "<p><strong>Embedding Layer (<code class=\"language-plaintext highlighter-rouge\">embed_tokens</code>)</strong>: Converts input DNA sequences into dense vectors of fixed size. It maps each of the 4,096 (\\(4^{6}\\)) possible DNA tokens (representing 6-mers) to a 256-dimensional vector space. This embedding layer is crucial for transforming discrete DNA sequences into a format suitable for neural network processing.</p>\n",
    "</li>\n",
    "<li><strong>Decoder Layers (<code class=\"language-plaintext highlighter-rouge\">layers</code>)</strong>: Consists of eight <code style=\"color: inherit\">MixtralDecoderLayer</code> modules, each containing several sub-components:\n",
    "<ul>\n",
    "<li>\n",
    "<p><strong>Self-Attention Mechanism (<code class=\"language-plaintext highlighter-rouge\">self_attn</code>)</strong></p>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-6\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<ol>\n",
    "<li>What are the components?</li>\n",
    "<li>How is the purpose?</li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-6\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-6\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>The components are linear projections (<code class=\"language-plaintext highlighter-rouge\">q_proj</code>, <code style=\"color: inherit\">k_proj</code>,<code class=\"language-plaintext highlighter-rouge\">v_proj</code>, <code style=\"color: inherit\">o_proj</code>) for queries, keys, values, and outputs, along witha rotary embedding (<code class=\"language-plaintext highlighter-rouge\">rotary_emb</code>) to incorporate positiona linformation.</li>\n",
    "<li>This allows the model to weigh the importance of differenttokens in the sequence relative to each other, capturing dependenciesand context.</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>Sparse Mixture of Experts (<code class=\"language-plaintext highlighter-rouge\">block_sparse_moe</code>)</strong>:</p>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-7\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<ol>\n",
    "<li>What are the components?</li>\n",
    "<li>How is the purpose?</li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-7\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-7\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>The components are gating mechanism (<code class=\"language-plaintext highlighter-rouge\">gate</code>) and list of 64 expert networks (<code class=\"language-plaintext highlighter-rouge\">experts</code>), each with multiple linear layers (<code class=\"language-plaintext highlighter-rouge\">w1</code>, <code style=\"color: inherit\">w2</code>, <code style=\"color: inherit\">w3</code>) and an activation function (<code class=\"language-plaintext highlighter-rouge\">act_fn</code>).</li>\n",
    "<li>This efficiently processes input data by activating only a subset of expert networks, reducing computational load while maintaining model capacity.</li>\n",
    "</ol>\n",
    "</blockquote>\n",
    "</blockquote>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>Layer Normalization (<code class=\"language-plaintext highlighter-rouge\">input_layernorm</code>, <code style=\"color: inherit\">post_attention_layernorm</code>)</strong>: Stabilizes and accelerates the training process by normalizing the inputs and outputs of the attention mechanism.</p>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>Final Layer Normalization (<code class=\"language-plaintext highlighter-rouge\">norm</code>)</strong>: Applies normalization to the output of the final decoder layer, ensuring stable and consistent outputs.</p>\n",
    "</li>\n",
    "<li><strong>Language Model Head (<code class=\"language-plaintext highlighter-rouge\">lm_head</code>)</strong>: Projects the 256-dimensional output of the final decoder layer back into the 4,096-dimensional vocabulary space of DNA tokens. This linear layer (<code class=\"language-plaintext highlighter-rouge\">Linear</code>) maps the hidden states to the original token space, enabling the model to predict the next DNA token accurately.</li>\n",
    "</ol>\n",
    "<p>This architecture ensures that the model can capture complex patterns in DNA sequences while maintaining computational efficiency, making it suitable for tasks like DNA sequence generation and analysis. The model‚Äôs design culminates in the output of 4,096 tokens, aligning with the input dimension. This consistency is crucial for accurately predicting the next token in a given DNA sequence, ensuring that the model‚Äôs predictions are coherent and reliable.</p>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-8\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>How many parameters are in this model?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-8\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-8\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div><pre style=\"color: inherit; background: transparent\"><code style=\"color: inherit\">pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model size: {pytorch_total_params/1000**2:.1f}M parameters\")\n",
    "</code></pre></div>    </div>\n",
    "<p>There are 105 millions parameters. It is a big model.</p>\n",
    "</blockquote>\n",
    "</blockquote>\n",
    "<h1 id=\"prepare-the-tokenizer\">Prepare the tokenizer</h1>\n",
    "<p>A tokenizer is a crucial component in natural language processing (NLP) that transforms raw text into a format that can be processed by machine learning models. In this section, we will load and configure the <strong>Byte-Pair Encoding (BPE) letter tokenizer</strong>. The BPE tokenizer efficiently handles rare and unknown words by breaking them down into frequent subword units, ensuring that the model can generalize better to unseen data. This process involves initializing the tokenizer with a predefined vocabulary and settings, enabling it to convert text into a format suitable for neural network processing. By doing so, we prepare the tokenizer to effectively manage DNA sequences, facilitating accurate and reliable model predictions.</p>\n",
    "<p>Let‚Äôs loads a pre-trained tokenizer from the Hugging Face Model Hub. The tokenizer is associated with the model <code style=\"color: inherit\">DNABERT-2-117M</code>, which is designed for processing DNA sequences.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57700d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efa9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82df64dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='zhihan1996/DNABERT-2-117M', vocab_size=4096, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a25f8f",
   "metadata": {},
   "source": [
    "</code></pre></div></div>\n",
    "<p>The <code style=\"color: inherit\">PreTrainedTokenizerFast</code> is a fast and efficient tokenizer used to process text data for the <code style=\"color: inherit\">DNABERT-2-117M</code> model. Here‚Äôs a breakdown of its configuration:</p>\n",
    "<ul>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">name_or_path='zhihan1996/DNABERT-2-117M'</code>: Specifies the name or path of the pre-trained tokenizer, indicating that it is associated with the <code style=\"color: inherit\">DNABERT-2-117M</code> model, which is designed for processing DNA sequences.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">vocab_size=4096</code>: Defines the size of the tokenizer‚Äôs vocabulary.</p>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-10\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>Why is the size of the tokenizer‚Äôs vocabulary set to 4,096?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-10\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-10\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>It corresponds to the number of unique tokens (6-mers) that the model can recognize in DNA sequences.</p>\n",
    "</details>\n",
    "</blockquote>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">special_tokens</code>: Defines a set of special tokens used by the tokenizer:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">unk_token: '[UNK]'</code> - Represents unknown or out-of-vocabulary tokens.</li>\n",
    "<li><code style=\"color: inherit\">sep_token: '[SEP]'</code> - Used to separate segments within a sequence.</li>\n",
    "<li><code style=\"color: inherit\">pad_token: '[PAD]'</code> - Used for padding sequences to a uniform length.</li>\n",
    "<li><code style=\"color: inherit\">cls_token: '[CLS]'</code> - Typically used as the first token in a sequence to represent the classification token.</li>\n",
    "<li><code style=\"color: inherit\">mask_token: '[MASK]'</code> - Used in masked language modeling to hide tokens that the model must predict.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-11\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the other configuration parameters mean?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">model_max_length=1000000000000000019884624838656</code></li>\n",
    "<li><code style=\"color: inherit\">is_fast=True</code></li>\n",
    "<li><code style=\"color: inherit\">padding_side='right'</code></li>\n",
    "<li><code style=\"color: inherit\">truncation_side='right'</code></li>\n",
    "<li><code style=\"color: inherit\">clean_up_tokenization_spaces=False</code></li>\n",
    "<li><code style=\"color: inherit\">added_tokens_decoder</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-11\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-11\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">model_max_length=1000000000000000019884624838656</code>: Represents the maximum length of sequences that the model can handle.</p>\n",
    "<p>This extremely large value suggests that the model is designed to process very long sequences, although in practice, the actual limit will be constrained by available computational resources.</p>\n",
    "</li>\n",
    "<li><code style=\"color: inherit\">is_fast=True</code>: Indicates that this tokenizer is optimized for speed, leveraging Rust-based implementations to accelerate tokenization processes.</li>\n",
    "<li><code style=\"color: inherit\">padding_side='right'</code>: Configures the tokenizer to pad sequences on the right side, ensuring that all sequences in a batch have the same length by adding padding tokens to the end of shorter sequences.</li>\n",
    "<li><code style=\"color: inherit\">truncation_side='right'</code>: Specifies that sequences will be truncated from the right side if they exceed the maximum length, preserving the beginning of the sequence.</li>\n",
    "<li><code style=\"color: inherit\">clean_up_tokenization_spaces=False</code>: Indicates that the tokenizer will not remove spaces after tokenization, preserving the original spacing in the text.</li>\n",
    "<li><code style=\"color: inherit\">added_tokens_decoder</code>: Maps token IDs to their corresponding <code style=\"color: inherit\">AddedToken</code> objects, which include metadata such as whether the token is a special token and how it should be processed (e.g., stripping whitespace).</li>\n",
    "</ol>\n",
    "</blockquote>\n",
    "</blockquote>\n",
    "<p>This configuration ensures that the tokenizer is tailored to efficiently process DNA sequences, handling both the tokenization and padding/truncation of sequences in a manner that aligns with the model‚Äôs requirements.</p>\n",
    "<p>By default, tokenizers may pad sequences on the right side (<code class=\"language-plaintext highlighter-rouge\">padding_side='right'</code>). Let‚Äôs set the padding direction for the tokenizer.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1e515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side  = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5110e58f",
   "metadata": {},
   "source": [
    "<p>When tokenizing a batch of sequences, shorter sequences will be padded with special tokens on the left to match the length of the longest sequence in the batch. This can be useful for ensuring consistent input sizes, especially in models that expect fixed-size inputs.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79f0135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 2061,    2]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"ATT\", padding=\"longest\", return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66698db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 2061,  281,  485,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "## encoding for maximum token length\n",
    "encoding = tokenizer(\"ATTGTGGGTCCCCGTAGATGATAGGGGCCCCCC\", max_length=5, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b812b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_text = load_dataset(\"csv\", data_files=\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de5ebb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 99999\n",
      "    })\n",
      "})\n",
      "TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCCTAACCCTAACCCTAACCTAACCCTAACCCTAACCCTAA\n",
      "200\n",
      "99999\n"
     ]
    }
   ],
   "source": [
    "print(dataset_text)\n",
    "print(dataset_text['train']['text'][0])\n",
    "print(len(dataset_text['train']['text'][0]))\n",
    "print(len(dataset_text['train']['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0ebd9",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58015115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916f2d8",
   "metadata": {},
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-14\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the following parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">padding=\"longest\"</code></li>\n",
    "<li><code style=\"color: inherit\">truncation=True</code></li>\n",
    "<li><code style=\"color: inherit\">return_tensors=\"pt\"</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-14\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-14\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">padding=\"longest\"</code> ensures that all sequences in the batch are padded to the length of the longest sequence, adding padding tokens as needed.</li>\n",
    "<li><code style=\"color: inherit\">truncation=True</code> specifies that sequences exceeding the model‚Äôs maximum length will be truncated to fit.</li>\n",
    "<li><code style=\"color: inherit\">return_tensors=\"pt\"</code> indicates that the output should be in the form of PyTorch tensors, suitable for use with PyTorch-based models.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6573d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_text.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eb42442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 99999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3eb9ff",
   "metadata": {},
   "source": [
    "</code></pre></div>        </div>\n",
    "<p><code style=\"color: inherit\">dataset</code> is a <code style=\"color: inherit\">DatasetDict</code> with 1 <code style=\"color: inherit\">train</code> <code style=\"color: inherit\">Dataset</code> made of 99,999 rows and 4 features:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">text</code>: The original text data before tokenization.</li>\n",
    "<li><code style=\"color: inherit\">input_ids</code>: The tokenized input data, represented as numerical IDs.</li>\n",
    "<li><code style=\"color: inherit\">token_type_ids</code>: Indicates the type of each token, useful for models that handle multiple segments.</li>\n",
    "<li><code style=\"color: inherit\">attention_mask</code>: Specifies which tokens should be attended to by the model (<code style=\"color: inherit\">1</code> for real tokens, <code style=\"color: inherit\">0</code> for padding).</li>\n",
    "</ul>\n",
    "</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de478502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "[888, 956, 102, 615, 79, 956, 338, 956, 956, 956]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['input_ids'][1][0:10])\n",
    "print(dataset['train']['input_ids'][1][32:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ee9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['token_type_ids'][1][0:10])\n",
    "print(dataset['train']['token_type_ids'][1][32:42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91a72fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train']['attention_mask'][1][0:10])\n",
    "print(dataset['train']['attention_mask'][1][32:42])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c532f3",
   "metadata": {},
   "source": [
    "<li>The first tokenized sequence of <code style=\"color: inherit\">train</code> <code style=\"color: inherit\">Dataset</code> (<code class=\"language-plaintext highlighter-rouge\">dataset[\"train\"][1]</code>) is a dictionary with:\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">text</code>: 200 base pair sequence</li>\n",
    "<li><code style=\"color: inherit\">input_ids</code>: list of 49 numerical values, the token IDs.</li>\n",
    "<li><code style=\"color: inherit\">token_type_ids</code>: list 49 <code style=\"color: inherit\">0</code></li>\n",
    "<li><code style=\"color: inherit\">attention_mask</code>: list of 7 <code style=\"color: inherit\">0</code> (padding) and 42 <code style=\"color: inherit\">1</code> (real tokens)</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a67e95ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data splitting\n",
    "train_size = int(0.8 * len(dataset[\"train\"]))\n",
    "val_size = len(dataset[\"train\"]) - train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b73e2842",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset[\"train\"], [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49a45d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 99999\n",
      "}) 79999\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 99999\n",
      "}) 20000\n"
     ]
    }
   ],
   "source": [
    "print(train_set.dataset, len(train_set.indices))\n",
    "print(val_set.dataset, len(val_set.indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e6386f",
   "metadata": {},
   "source": [
    "<h2 id=\"data-collation\">Data Collation</h2>\n",
    "<p>The <code style=\"color: inherit\">DataCollatorForLanguageModeling</code> is a utility class, designed to prepare and format batches of data for language modeling tasks. It handles the dynamic padding and masking of input sequences, ensuring that each batch fed into the model is correctly formatted and optimized for training.</p>\n",
    "\n",
    "A data collator is a function/class that:\n",
    "<li>Takes a list of tokenized samples\n",
    "<li>Pads them to the same length\n",
    "<li>Creates the final tensors (input_ids, attention_mask, labels)\n",
    "<li>Prepares them correctly for training\n",
    "<li>It runs at batch time, right before data is fed to the model.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2960ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# tokenizer convert the raw text into the numerical tokens\n",
    "# mlm= False, which is set up for causal language modeling (CLM) rather than masked MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8853cf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='zhihan1996/DNABERT-2-117M', vocab_size=4096, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bd30bc",
   "metadata": {},
   "source": [
    "<p>This will:</p>\n",
    "<ol>\n",
    "<li>Automatically pads sequences within a batch to ensure they are of equal length, which is necessary for efficient batch processing in neural networks.</li>\n",
    "<li>Generates attention masks that indicate which tokens should be attended to by the model, ignoring padding tokens.</li>\n",
    "<li>Collates individual examples into batches, handling the necessary formatting and ensuring compatibility with the model‚Äôs input requirements.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282bce4",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "## Define parameters for pretraining\n",
    "\n",
    "</p> We are going to define the hyperparameters & configurations for training the language models using the Hugging Face transformers. </p>\n",
    "\n",
    "<p> Batch size implies total number of samples will be processed before the model weights. This should be chosen to balance the computational efficiency and memnory usage </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f4a9ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "batchsize = 128\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/models\", # model, checkpoints & results saved\n",
    "    eval_strategy=\"epoch\", # performance evaluate after each epoch\n",
    "    save_strategy=\"epoch\", # model save at each epoch\n",
    "    num_train_epochs=50, # Iterates entire dataset 50 times\n",
    "    per_device_train_batch_size=batchsize, # samples iterates based on batchsize\n",
    "    per_device_eval_batch_size=batchsize,\n",
    "    learning_rate=5e-4, \n",
    "    weight_decay=0.01, # L2 regularization to prevent overfitting\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True, \n",
    "    bf16=True,  # enables mixed-precision training using 16-bit floating-point numbers\n",
    "    gradient_accumulation_steps=50,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5fd55",
   "metadata": {},
   "source": [
    "**load_best_model_at_end**\n",
    "<p> ensures that the best model, based on the lowest evaluation loss, is loaded at the end of training. This helps in selecting the model with the best performance across all epochs. During gradient descent, the model will be optimized, and at some point, the loss will start to increase again. We want to pick the model with the lowest loss, not when it starts increasing. So, ‚Äúload best model at the end‚Äù means selecting the model with the best loss across all epochs </p>\n",
    "\n",
    "**gradient_accumulation_steps**\n",
    "<p>accumulates gradients over 50 steps before performing a backward pass. This effectively increases the batch size without requiring additional memory, helping to stabilize training.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff2aa0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb09ac",
   "metadata": {},
   "source": [
    "<p>With a patience of three, even if we find a good minimum, we wait for three more epochs to ensure that the loss does not improve further. If the loss does not decrease for three consecutive epochs, we stop training. However, if a better model with a lower loss is found within those three epochs, training continues. This approach helps in finding a more robust local minimum by reducing the impact of noise in the training data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea5ff9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5/600 01:22 < 4:33:27, 0.04 it/s, Epoch 0.32/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# I ran the whole training unfortunately it is not showing up, may be forgot to save. \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# The training stops at 20 epochs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# The loss is 6.09\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/accelerate/utils/operations.py:819\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/accelerate/utils/operations.py:807\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1332\u001b[0m, in \u001b[0;36mMixtralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\u001b[0m\n\u001b[1;32m   1329\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1332\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1345\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1346\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:1200\u001b[0m, in \u001b[0;36mMixtralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, output_router_logits, return_dict)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1190\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1191\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         use_cache,\n\u001b[1;32m   1198\u001b[0m     )\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_router_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_router_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:918\u001b[0m, in \u001b[0;36mMixtralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, output_router_logits, use_cache)\u001b[0m\n\u001b[1;32m    916\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 918\u001b[0m hidden_states, router_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_sparse_moe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    921\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:853\u001b[0m, in \u001b[0;36mMixtralSparseMoeBlock.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Index the correct hidden states and compute the expert hidden state for\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# the current expert. We need to make sure to multiply the output hidden\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# states by `routing_weights` on the corresponding tokens (top-1 and top-2)\u001b[39;00m\n\u001b[1;32m    852\u001b[0m current_state \u001b[38;5;241m=\u001b[39m hidden_states[\u001b[38;5;28;01mNone\u001b[39;00m, top_x]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_dim)\n\u001b[0;32m--> 853\u001b[0m current_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mexpert_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m routing_weights[top_x, idx, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# However `index_add_` only support torch tensors for indexing so we'll use\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# the `top_x` tensor here.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m final_hidden_states\u001b[38;5;241m.\u001b[39mindex_add_(\u001b[38;5;241m0\u001b[39m, top_x, current_hidden_states\u001b[38;5;241m.\u001b[39mto(hidden_states\u001b[38;5;241m.\u001b[39mdtype))\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/models/mixtral/modeling_mixtral.py:789\u001b[0m, in \u001b[0;36mMixtralBlockSparseTop2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m--> 789\u001b[0m     current_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw3(hidden_states)\n\u001b[1;32m    790\u001b[0m     current_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw2(current_hidden_states)\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m current_hidden_states\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# I ran the whole training unfortunately it is not showing up, may be forgot to save. \n",
    "# The training stops at 20 epochs\n",
    "# The loss is 6.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4473c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking previous trained Mistral DNA language model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"RaphaelMourad/Mistral-DNA-v1-17M-hg38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf7949",
   "metadata": {},
   "source": [
    "<p>This is a mixed model that was pre-trained on the entire Human Genome. It contains approximately 17 million parameters and was trained using the Human Genome assembly GRCh38. Unlike models pre-trained on sequences of 200 bases, this model was pre-trained on sequences of 10,000 bases (10K). The advantage of this model is its ability to process larger DNA contexts or sequences. This capability allows it to capture more extensive patterns and dependencies within the genomic data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5d01413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=256, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eb6a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = [\"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\",\"AAGGTTTCCCAAAAATTTTTACTCTACTCGGGCAGGCAGACGACGAGCTTCATATCAGCGAGCA\"]\n",
    "tokenized_dna = tokenizer(dna, max_length = 10, return_tensors = 'pt', padding=\"longest\", truncation=True)\n",
    "# Adding to truncate at the maximum length i.e. 10\n",
    "inputs = tokenized_dna[\"input_ids\"]\n",
    "model_outputs = model(inputs)\n",
    "hidden_states = model_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "987ec30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,    5,  194,   32,  757, 1239, 2092,  294,   24,    2],\n",
      "        [   1,    9,  101,   26,  175,  536,  436, 3189,  253,    2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f0577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([2, 10, 4096])\n"
     ]
    }
   ],
   "source": [
    "print(len(dna))\n",
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb01b3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4096])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dc979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd602c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb36ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7141938f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
