{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d87260e",
   "metadata": {},
   "source": [
    "DNA Foundation model for practice after learning the LLM NanoGPT / GPT2 from Dr. Andrej Karpathy and Mistral DNA. \n",
    "\n",
    "**References**\n",
    "\n",
    "<li> https://github.com/shreydan/makemore-series  </li>\n",
    "<li> https://github.com/karpathy/nanoGPT  </li>\n",
    "<li> https://github.com/raphaelmourad/Mistral-DNA  </li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e502c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# conda activate torch_gpu\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2c01ee",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "To pretrain a model, we will use a file containig 100,000 non-overlapping DNA sequences of 200 bases corresponding to around 1% of the human genome (hg38 assembly). Mistral DNA works on Causal Language modelling (CLM) just like GPT not like BERT that works on MLM (Masked language model). In CLM, the token is predicted from the previous tokens. While in MLM, the token is masked irrespective of the position and all the tokens.\n",
    "\n",
    "Tokenization is performed based on the Byte Pair Encoding (BPE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d059f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the dataset\n",
    "# Got the dataset from https://github.com/raphaelmourad/LLM-for-genomics-training\n",
    "# dataset_text = load_dataset(\"csv\", data_files=\"/mnt/data/projects/.immune/Personal/DNA-Language-Model/DNA_FM/data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")\n",
    "savedir = \"/mnt/data/projects/.immune/Personal/DNA-Language-Model/Mistral_DNA/\"\n",
    "os.chdir(savedir)\n",
    "import pandas as pd\n",
    "DNA_text = pd.read_csv(os.path.join(savedir,\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd964ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAGGAGAACGCAACTCCGCCGTTGCAAAGGCGCGCCGCGCCGGCGC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CACATGCTAGCGCGTCGGGGTGGAGGCGTGGCGCAGGCGCAGAGAG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAAC...\n",
       "1  CCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAACCCTAACCC...\n",
       "2  TAACCCTAACCCTAACCCTAACCCTAACCCTAACCCTAACCCCTAA...\n",
       "3  GAGGAGAACGCAACTCCGCCGTTGCAAAGGCGCGCCGCGCCGGCGC...\n",
       "4  CACATGCTAGCGCGTCGGGGTGGAGGCGTGGCGCAGGCGCAGAGAG..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNA_text[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da228f",
   "metadata": {},
   "source": [
    "### Performing Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd67ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84, 65, 65, 67, 67]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x)for x in DNA_text['text'][0]][0:5] ## These are basically the ASCII character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd74de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TAACC'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNA_joined = \"\".join(DNA_text['text'].tolist())\n",
    "DNA_joined[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27779440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "TAACCCTAAC\n",
      "length: 19999800\n",
      "---\n",
      "[84, 65, 65, 67, 67, 67, 84, 65, 65, 67]\n",
      "length: 19999800\n"
     ]
    }
   ],
   "source": [
    "tokens = DNA_joined.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(DNA_joined[0:10])\n",
    "print(\"length:\", len(DNA_joined))\n",
    "print('---')\n",
    "print(tokens[0:10])\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15c747c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{65, 67, 71, 84}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(max(tokens))\n",
    "set(tokens) # ATGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a62d2da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining the two character into one\n",
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "# print(stats)\n",
    "# print(sorted(((v,k) for k,v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62f988f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 65)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_pair = max(stats, key=stats.get)\n",
    "top_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be84245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    new_text = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) -1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "            new_text.append(idx)\n",
    "            i+=2\n",
    "        else:\n",
    "            new_text.append(ids[i])\n",
    "            i+=1\n",
    "    return new_text\n",
    "\n",
    "# print(merge([5,5,6,6,7,8,6,7,9],[6,7],99))\n",
    "\n",
    "# tokens2 = merge(tokens, top_pair, 256)\n",
    "# print(tokens2[0:10])\n",
    "# print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 4096 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 4 # since it is only A,T,G,C\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    print(i)\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key = stats.get)\n",
    "    idx = 256 + i\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08d1acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3721774 4093\n",
      "compression ratio: 5.37X\n"
     ]
    }
   ],
   "source": [
    "print(len(ids), len(set(ids)))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d89f24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    token = text.encode(\"utf-8\")\n",
    "    token = list(map(int, token))\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ATGGCCTTAACCCCCCTCTGCGAATTACCATTGGGAGTTTCACCC\"\n",
    "token_encoded = encode(text)\n",
    "print(len(token_encoded), len(text))\n",
    "print(token_encoded, \"\\n\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d65420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 45\n",
      "[308, 309, 259, 1493, 743, 337, 481, 293, 497, 256, 260, 67] \n",
      " ATGGCCTTAACCCCCCTCTGCGAATTACCATTGGGAGTTTCACCC\n"
     ]
    }
   ],
   "source": [
    "## Now UTF-8 is not 256 it is now 4096. \n",
    "def encode(text):\n",
    "  # given a string, return list of integers (the tokens)\n",
    "  tokens = list(text.encode(\"utf-8\"))\n",
    "  while len(tokens) >= 2:\n",
    "    stats = get_stats(tokens)\n",
    "    pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "    if pair not in merges:\n",
    "      break # nothing else can be merged\n",
    "    idx = merges[pair]\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "  return tokens\n",
    "\n",
    "token_encoded = encode(text)\n",
    "print(len(token_encoded), len(text))\n",
    "print(token_encoded, \"\\n\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "318cdcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde13730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'AACAAA', b'TGGGGT', b'CAGGGT', b'CTGCG'] [b'd', b'e', b'f', b'g']\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "value_voc =list(vocab.values())\n",
    "print(value_voc[740:744], value_voc[100:104])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952960b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8d91204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ATGG'\n",
      "b'CCTT'\n",
      "b'AA'\n",
      "b'CCCCCCT'\n",
      "b'CTGCG'\n",
      "b'AATT'\n",
      "b'ACCAT'\n",
      "b'TGGG'\n",
      "b'AGTTT'\n",
      "b'CA'\n",
      "b'CC'\n",
      "b'C'\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(token_encoded)):\n",
    "    print(vocab[token_encoded[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ac9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(token):\n",
    "    token_join = b''.join(vocab[idx] for idx in token)\n",
    "    # print(token_join)\n",
    "    text = token_join.decode(\"utf-8\", errors = \"replace\") ## output not valid so we replace\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3cd18ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATGGCCTTAACCCCCCTCTGCGAATTACCATTGGGAGTTTCACCC\n"
     ]
    }
   ],
   "source": [
    "text = decode(token_encoded)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92bdd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATGGCCTTAACC\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"ATGGCCTTAACC\")))\n",
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ba1f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'ATGGCCTTAACCCCCCTCTGCGAATTACCATTGGGAGTTTCACCC'\n"
     ]
    }
   ],
   "source": [
    "print(b''.join(vocab[idx] for idx in token_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a42c1e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(ids).to_csv(\"/mnt/data/projects/.immune/Personal/DNA-Language-Model/Mistral_DNA/data/genome_sequences/hg38/ids_encoded_2.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94a33fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/mnt/data/projects/.immune/Personal/DNA-Language-Model/Mistral_DNA/data/genome_sequences/hg38/vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(my_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b25d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_csv(os.path.join(savedir, \"data/genome_sequences/hg38/ids_encoded_2.csv\"), header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3932e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c471281b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4093"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa8359f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(\"data/genome_sequences/hg38/vocab.pkl\"), \"rb\") as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2144e377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 1, 'B': [1, 2, 3], 'C': {'x': 10}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "12febd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 31, 12, 3]\n",
      "['[CLS]', 'ACGTAC', 'GT', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "sequences = [\n",
    "    \"ACGTACGTACGT\",\n",
    "    \"CGTACGTACGTA\",\n",
    "    \"ATATATATAT\"\n",
    "]\n",
    "\n",
    "tokenizer = DNABert2LikeTokenizer(max_len=6)\n",
    "tokenizer.train(sequences)\n",
    "\n",
    "encoded = tokenizer.encode(\"ACGTACGT\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5914f2d",
   "metadata": {},
   "source": [
    "## DNA GPT Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf12726",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_t = torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae600f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have tried a configuration of 2**n makes it more efficients\n",
    "@dataclass\n",
    "class DNAGPTconfig:\n",
    "    block_size: int = 1024 ## it is the token size\n",
    "    n_layer: int = 16\n",
    "    embd_size: int = 512\n",
    "    n_head: int = 16\n",
    "    vocab_size: int = (ids_t.max() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de71158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.embd_size, 3 * config.embd_size) # 3 dimension as it is divided into q,k,v\n",
    "        self.c_proj = nn.Linear(config.embd_size, config.embd_size)\n",
    "        self.n_head = config.n_head\n",
    "        self.embd_size = config.embd_size\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.embd_size, dim=2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1,2)\n",
    "        # wei = q @ k.transpose(-2,-1)\n",
    "        # wei = wei * C**-0.5\n",
    "        # wei = F.softmax(wei, dim = -1) \n",
    "        # wei = self.dropout(wei)\n",
    "        # wei = wei @ v\n",
    "        # Instead of running all of them, we can use flash attention at once\n",
    "        wei = F.scaled_dot_product_attention(q,k,v, is_causal = True) # flash attention\n",
    "        # combine all of them\n",
    "        wei = wei.transpose(1,2).contiguous().view(B,T,C)\n",
    "        wei = self.c_proj(wei)\n",
    "        return wei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5e63258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.Linear(config.embd_size, 4 * config.embd_size)\n",
    "        self.nln = nn.GELU(approximate = \"tanh\")\n",
    "        self.ln2 = nn.Linear(4 * config.embd_size, config.embd_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x)\n",
    "        x = self.nln(x)\n",
    "        x = self.ln2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8329049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f1 = nn.LayerNorm(config.embd_size)\n",
    "        self.self_attn = CausalSelfAttention(config)\n",
    "        self.ln_f2 = nn.LayerNorm(config.embd_size)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attn(self.ln_f1(x))\n",
    "        x = x + self.mlp(self.ln_f2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e73d3cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNAGPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformers = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.embd_size),\n",
    "            wpe = nn.Embedding(config.block_size, config.embd_size),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_norm = nn.LayerNorm(config.embd_size)\n",
    "            )\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.embd_size, config.vocab_size, bias = False)\n",
    "    \n",
    "    def forward(self, idx, targets = None):\n",
    "        B,T = idx.shape\n",
    "        tok = self.transformers.wte(idx)\n",
    "        pos = self.transformers.wpe(torch.arange(T, dtype = torch.long, device = idx.device))\n",
    "        x = tok + pos ## require both the position and token\n",
    "        for block in self.transformers.h:\n",
    "            x=block(x) ## Since it will go through all the layers of the transformers\n",
    "        x=self.transformers.ln_norm(x)\n",
    "        logits=self.lm_head(x)\n",
    "        loss = None\n",
    "\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) # neg log likelihood\n",
    "\n",
    "        return logits,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "248d5c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAGPT(\n",
       "  (transformers): ModuleDict(\n",
       "    (wte): Embedding(4348, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (nln): GELU(approximate='tanh')\n",
       "          (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DNAGPT(DNAGPTconfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75f0ebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name cuda\n"
     ]
    }
   ],
   "source": [
    "## Now for training you surely need to have GPU or cuda\n",
    "# device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends,\"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "\n",
    "print(\"device name\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f6d71ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAGPT(\n",
       "  (transformers): ModuleDict(\n",
       "    (wte): Embedding(4348, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (nln): GELU(approximate='tanh')\n",
       "          (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will pass all the weights and bias from gpt2 to our model\n",
    "model = DNAGPT(DNAGPTconfig())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a24e1495",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9 * len(ids))\n",
    "train_ids = ids[:split]\n",
    "val_ids   = ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c288b6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3349596 372178\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ids), len(val_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f0e483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def nextbatch(self, ids):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        ids = torch.tensor(ids)\n",
    "        buf = ids[self.current_position : self.current_position+B*T+1]\n",
    "        x = buf[:-1].view(B,T)\n",
    "        y = buf[1:].view(B,T)\n",
    "        self.current_position += B * T + 1\n",
    "        # if loading the last batch is greater than the lenght\n",
    "        if (self.current_position + (B * T + 1) > len(ids)):\n",
    "            self.current_position = 0\n",
    "        x=x.to(device) ## putting it on GPU\n",
    "        y=y.to(device)\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "da513947",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "d = DataLoaderLite(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2a7582fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAGPT(\n",
       "  (transformers): ModuleDict(\n",
       "    (wte): Embedding(4348, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (nln): GELU(approximate='tanh')\n",
       "          (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c940754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1432e029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0792996808886528\n"
     ]
    }
   ],
   "source": [
    "model.train() # In Training mode\n",
    "for _ in range(1000):\n",
    "    x, y = d.nextbatch(ids[:192 + 1])\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b0cedde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() ## So it should not update any weights\n",
    "\n",
    "def val_step():\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for _ in range(100):\n",
    "        x, y = d.nextbatch(val_ids[:192 + 1])\n",
    "        logits, lossing = model(x, targets = None)\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T,C)\n",
    "        targets = y.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets) # neg log likelihood\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5849e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.109191064834596\n"
     ]
    }
   ],
   "source": [
    "print(val_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "57425e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000 % 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() ## So it should not update any weights\n",
    "\n",
    "def val_step2():\n",
    "    model.eval()\n",
    "    losses=[]\n",
    "    for _ in range(10):\n",
    "        x, y = d.nextbatch(val_ids[:192 + 1])\n",
    "        logits, loss = model(x, targets = None)\n",
    "        # B, T, C = logits.shape\n",
    "        # logits = logits.view(B*T,C)\n",
    "        # targets = y.view(B*T)\n",
    "        # loss = F.cross_entropy(logits, targets) # neg log likelihood\n",
    "        # losses.append(loss.item())\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37c2b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4567f0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 8, 4348])\n"
     ]
    }
   ],
   "source": [
    "logistic, loss = val_step2()\n",
    "print(logistic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dd35c46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 8, 4348])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00995bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 4348])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd722a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  84, 1331, 1331, 1331, 1331, 1331, 1331, 1331],\n",
      "        [1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331],\n",
      "        [1331, 1331, 1083,  296,  362,  314, 1331, 1331],\n",
      "        [1331, 1331, 2846, 1331, 1331, 1331, 1331,  681],\n",
      "        [1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331],\n",
      "        [2846, 1331, 1331,  682,  362,  296,  362,  314],\n",
      "        [1331, 1331, 1331, 1976,  830,  830,  830,  830],\n",
      "        [ 830,  296,  362,  368, 1331, 1331, 1331,  296],\n",
      "        [ 362,  314, 1331, 1331, 1331, 1331, 2846, 2846],\n",
      "        [1331, 1331, 1331, 1331, 1331, 1331, 2846, 1331],\n",
      "        [1331, 1331, 1331,  271,  496, 3393,  352,  349],\n",
      "        [ 349, 2293,  264,  405, 1803, 1949, 4094,  881],\n",
      "        [ 325,  292,  355, 3649, 1105, 3220,  642,  314],\n",
      "        [ 824,  972, 1404, 3070, 1803,  271,  621,  292],\n",
      "        [ 383, 3522,  539,  834, 3093, 1211, 3828,  539],\n",
      "        [ 834, 3093, 1211, 3828,  539,  834, 3093, 1211],\n",
      "        [3828,  539,  834, 3093, 1211, 3828,  539,  834],\n",
      "        [3093, 1211, 3828,  539,  834, 3093, 1211,  811],\n",
      "        [1356,  552,  324,  428,  744,  487,  271, 1211],\n",
      "        [3828,  539,  834, 3093, 1211,  386,  273, 1356],\n",
      "        [ 479,  324,  767,  744,  487,  271, 1211,  386],\n",
      "        [1176,  674,  271, 3093, 1211,  386,  273, 1356],\n",
      "        [ 552,  324,  767,  744,  487,  271, 1211,  386],\n",
      "        [ 333,  321,  272,  655, 3775, 3205,  271,  464]]) \n",
      " tensor([[1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331],\n",
      "        [1331, 1331, 1331, 1331, 1331, 1331, 1331, 1331],\n",
      "        [1331, 1083,  296,  362,  314, 1331, 1331, 1331],\n",
      "        [1331, 2846, 1331, 1331, 1331, 1331,  681, 1331],\n",
      "        [1331, 1331, 1331, 1331, 1331, 1331, 1331, 2846],\n",
      "        [1331, 1331,  682,  362,  296,  362,  314, 1331],\n",
      "        [1331, 1331, 1976,  830,  830,  830,  830,  830],\n",
      "        [ 296,  362,  368, 1331, 1331, 1331,  296,  362],\n",
      "        [ 314, 1331, 1331, 1331, 1331, 2846, 2846, 1331],\n",
      "        [1331, 1331, 1331, 1331, 1331, 2846, 1331, 1331],\n",
      "        [1331, 1331,  271,  496, 3393,  352,  349,  349],\n",
      "        [2293,  264,  405, 1803, 1949, 4094,  881,  325],\n",
      "        [ 292,  355, 3649, 1105, 3220,  642,  314,  824],\n",
      "        [ 972, 1404, 3070, 1803,  271,  621,  292,  383],\n",
      "        [3522,  539,  834, 3093, 1211, 3828,  539,  834],\n",
      "        [3093, 1211, 3828,  539,  834, 3093, 1211, 3828],\n",
      "        [ 539,  834, 3093, 1211, 3828,  539,  834, 3093],\n",
      "        [1211, 3828,  539,  834, 3093, 1211,  811, 1356],\n",
      "        [ 552,  324,  428,  744,  487,  271, 1211, 3828],\n",
      "        [ 539,  834, 3093, 1211,  386,  273, 1356,  479],\n",
      "        [ 324,  767,  744,  487,  271, 1211,  386, 1176],\n",
      "        [ 674,  271, 3093, 1211,  386,  273, 1356,  552],\n",
      "        [ 324,  767,  744,  487,  271, 1211,  386,  333],\n",
      "        [ 321,  272,  655, 3775, 3205,  271,  464,  671]])\n"
     ]
    }
   ],
   "source": [
    "### testing to see if this work\n",
    "buf = torch.tensor(ids[:192 + 1]) # one for target\n",
    "x = buf[:-1].view(24,8)\n",
    "y = buf[1:].view(24,8)\n",
    "print(x,\"\\n\",y)\n",
    "x=x.to(device)\n",
    "y=y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a723d434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAGPT(\n",
       "  (transformers): ModuleDict(\n",
       "    (wte): Embedding(4348, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (nln): GELU(approximate='tanh')\n",
       "          (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will pass all the weights and bias from gpt2 to our model\n",
    "model = DNAGPT(DNAGPTconfig())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd85abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits,loss = model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b7060c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.4052, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f643677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9df3d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0823088064789772\n"
     ]
    }
   ],
   "source": [
    "for steps in range(100):\n",
    "    logits, loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "### We have overfit the model since we are using a single batch. So it learns everything\n",
    "## We will updae it in the batches using DataLoader that changes the batches randomly for it to generalize well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf779e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3721774"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ced632",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def nextbatch(self, ids):\n",
    "        B = self.B\n",
    "        T = self.T\n",
    "        ids = torch.tensor(ids)\n",
    "        buf = ids[self.current_position : self.current_position+B*T+1]\n",
    "        x = buf[:-1].view(B,T)\n",
    "        y = buf[1:].view(B,T)\n",
    "        self.current_position += B * T + 1\n",
    "        # if loading the last batch is greater than the lenght\n",
    "        if (self.current_position + (B * T + 1) > len(ids)):\n",
    "            self.current_position = 0\n",
    "        x=x.to(device) ## putting it on GPU\n",
    "        y=y.to(device)\n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e27e3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9 * len(ids))\n",
    "train_ids = ids[:split]\n",
    "val_ids   = ids[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7b502b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DataLoaderLite(16,DNAGPTconfig.block_size)\n",
    "# val_loader = DataLoaderLite(16,DNAGPTconfig.block_size)\n",
    "# x,y = d.nextbatch()\n",
    "# print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d59ba94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNAGPT(\n",
       "  (transformers): ModuleDict(\n",
       "    (wte): Embedding(4348, 512)\n",
       "    (wpe): Embedding(1024, 512)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (nln): GELU(approximate='tanh')\n",
       "          (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This will pass all the weights and bias from gpt2 to our model\n",
    "model = DNAGPT(DNAGPTconfig())\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35e6cca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4897894859313965\n",
      "7.39863395690918\n",
      "7.3050737380981445\n",
      "7.250854969024658\n",
      "7.307278633117676\n",
      "7.214249610900879\n",
      "7.238442897796631\n",
      "7.284749507904053\n",
      "7.297162055969238\n",
      "7.394287586212158\n",
      "7.199598789215088\n",
      "7.128458499908447\n",
      "7.172311782836914\n",
      "7.1416239738464355\n",
      "7.016969203948975\n",
      "7.126737594604492\n",
      "6.961976528167725\n",
      "7.029718399047852\n",
      "7.576591491699219\n",
      "7.290907859802246\n",
      "7.022881507873535\n",
      "7.155508995056152\n",
      "7.075373649597168\n",
      "7.07481050491333\n",
      "7.047640800476074\n",
      "7.051405429840088\n",
      "7.029366970062256\n",
      "6.996538162231445\n",
      "6.9593024253845215\n",
      "7.092647552490234\n",
      "7.07595157623291\n",
      "7.297584533691406\n",
      "7.218262195587158\n",
      "7.260923862457275\n",
      "7.181478023529053\n",
      "7.200864315032959\n",
      "7.255090713500977\n",
      "7.162593364715576\n",
      "7.179099082946777\n",
      "7.173696517944336\n",
      "7.153949737548828\n",
      "7.111686706542969\n",
      "7.13004207611084\n",
      "7.139341354370117\n",
      "7.132724761962891\n",
      "7.135664463043213\n",
      "7.141758441925049\n",
      "7.115208148956299\n",
      "7.132131099700928\n",
      "7.122882843017578\n",
      "7.128759384155273\n",
      "7.106016635894775\n",
      "7.123307228088379\n",
      "7.1655802726745605\n",
      "7.15963077545166\n",
      "7.1563920974731445\n",
      "7.142733573913574\n",
      "7.222889423370361\n",
      "7.225520133972168\n",
      "7.304229259490967\n",
      "7.206290245056152\n",
      "7.36350154876709\n",
      "7.206042289733887\n",
      "7.300286293029785\n",
      "7.31549072265625\n",
      "7.354308128356934\n",
      "7.2533769607543945\n",
      "7.157695770263672\n",
      "7.1827497482299805\n",
      "7.181609153747559\n",
      "7.213608264923096\n",
      "7.162977695465088\n",
      "7.1420769691467285\n",
      "7.109847545623779\n",
      "7.108219623565674\n",
      "7.1201677322387695\n",
      "7.18189001083374\n",
      "7.19350004196167\n",
      "7.2729387283325195\n",
      "7.263826370239258\n",
      "7.238306522369385\n",
      "7.27465295791626\n",
      "7.310573101043701\n",
      "7.192601203918457\n",
      "7.162328720092773\n",
      "7.17639684677124\n",
      "7.146316051483154\n",
      "7.177720546722412\n",
      "7.195863723754883\n",
      "7.278636932373047\n",
      "7.2874860763549805\n",
      "7.2447638511657715\n",
      "7.267457008361816\n",
      "7.249232292175293\n",
      "7.25663423538208\n",
      "7.227703094482422\n",
      "7.304933071136475\n",
      "7.273203372955322\n",
      "7.284622669219971\n",
      "7.254388332366943\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    x,y = d.nextbatch(ids)\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c9b974",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# training for more steps upto 100 \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     x,y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loader\u001b[49m\u001b[38;5;241m.\u001b[39mnextbatch(ids)\n\u001b[1;32m      4\u001b[0m     logits,loss \u001b[38;5;241m=\u001b[39m model(x,y)\n\u001b[1;32m      5\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# training for more steps upto 100 \n",
    "for _ in range(100):\n",
    "    x,y = train_loader.nextbatch(ids)\n",
    "    logits,loss = model(x,y)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51199d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda200c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c292df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee43fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5875b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "269daab2",
   "metadata": {},
   "source": [
    "# Mistral DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe4cbd",
   "metadata": {},
   "source": [
    "<p><strong>Generative Artificial Intelligence</strong> (AI) represents a cutting-edge domain within machine learning, focused on creating new, synthetic yet realistic data. This includes generating text, images, music, and even biological sequences. At the heart of many generative AI applications are <strong>Large Language Models</strong> (LLMs), which have revolutionized natural language processing and beyond.</p>\n",
    "<p>LLMs are <strong>sophisticated neural networks</strong> trained on vast amounts of text data to understand, generate, and interact with human language. Their architecture, often based on <strong>Transformers</strong>, allows them to capture complex patterns and context within data, making them powerful tools for various applications, from chatbots to creative writing and scientific discovery.</p>\n",
    "<blockquote class=\"details\" style=\"border: 2px solid #ddd; margin: 1em 0.2em\">\n",
    "<div class=\"box-title details-title\" id=\"details-transformers\"><button class=\"gtn-boxify-button details\" type=\"button\" aria-controls=\"details-transformers\" aria-expanded=\"true\"><i class=\"fas fa-info-circle\" aria-hidden=\"true\" ></i> <span>Details:  Transformers </span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>Transformers are a type of neural network model designed to handle sequential data, such as text, by using self-attention mechanisms to weigh the importance of input elements relative to each other, enabling the model to understand and generate coherent and contextually relevant outputs.</p>\n",
    "</blockquote>\n",
    "<p>In this tutorial, we will explore the intersection of generative AI and genomics by <strong>pretraining an LLM from scratch on DNA sequences</strong>. This process will equip the model with a foundational understanding of the grammar of DNA, enabling it to generate and analyze genetic data with remarkable accuracy.</p>\n",
    "<p><a href=\"https://mistral.ai/\">Mistral AI</a>, French artificial intelligence (AI) startup, recently launched large language models (LLMs) showing performances superior to Llama2. In particular, Mixtral-8x7B implements:</p>\n",
    "<ul>\n",
    "<li><strong>Grouped-Query Attention</strong>: Efficiently computes attention by grouping queries, reducing computational load and memory usage.</li>\n",
    "<li><strong>Sliding-Window Attention</strong>: Focuses on a fixed-size window of tokens, sliding over the sequence to manage long texts efficiently.</li>\n",
    "<li><strong>Byte-fallback BPE Tokenizer</strong>: Tokenizes text into subword units, falling back to byte-level tokenization for unknown words, ensuring robust handling of diverse text inputs.</li>\n",
    "</ul>\n",
    "<p>These techniques collectively enhance the performance and efficiency of large language models, enabling them to process and generate text more effectively.</p>\n",
    "<p>In this tutorial, we will use a simplified Mistral model architecture with fewer layers and hidden units to reduce computational requirements. The model will be trained to predict the next base in the sequence. For instance, for a sequence like <code style=\"color: inherit\">ATTTGTTGGT</code>, the model will be trained to predict the suffix <code style=\"color: inherit\">TTGGT</code> given the prefix <code style=\"color: inherit\">ATTTG</code>. This process is called <strong>causal language modeling</strong>.</p>\n",
    "<p>To pretrain the model, we will use a file containing 100,000 non-overlapping DNA sequences of 200 bases, corresponding to around 1% of the human genome (hg38 assembly). This involves training the model to predict the end of a DNA sequence.</p>\n",
    "<p>By the end of this tutorial, we will obtain a Mistral-DNA model with an internal representation of DNA sequence grammar. This pretrained model can then be used for various applications, such as fine-tuning for classification tasks or predicting mutational effects.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e3598",
   "metadata": {},
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What are the required dependencies doing?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary> View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ul>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">accelerate</code>: A library by <a href=\"https://huggingface.co/\">Hugging Face</a>  a platform that provides tools and resources for building, training, and deploying machine learning models  designed to simplify the process of training and deploying machine learning models across different hardware environments. It provides tools to optimize performance on GPUs, TPUs, and other accelerators, making it easier to scale models efficiently.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">datasets</code>: A library by Hugging Face for managing and processing datasets. It provides tools to load, manipulate, and share datasets in a standardized format, making it easier to work with machine learning data.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">numpy</code>: A fundamental package for scientific computing in Python.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">torch</code>: Also known as PyTorch, it is an open-source machine learning library developed by Facebooks AI Research lab. It provides a flexible platform for building and training neural networks, with a focus on tensor computations and automatic differentiation.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">transformers</code>: A library by Hugging Face that provides implementations of state-of-the-art transformer models for natural language processing (NLP). It includes pre-trained models and tools for fine-tuning, making it easier to apply transformers to various NLP tasks.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">flash-attn</code>: Implementation of FlashAttention, a Fast and Memory-Efficient Exact Attention with IO-Awareness\n",
    "These libraries are widely used in the machine learning and data science communities for their efficiency, flexibility, and extensive functionality.</p>\n",
    "</li>\n",
    "</ul>\n",
    "</details>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3be337",
   "metadata": {},
   "source": [
    "<blockquote class=\"details\" style=\"border: 2px solid #ddd; margin: 1em 0.2em\">\n",
    "<div class=\"box-title details-title\" id=\"details-loaded-functions-and-classes-from-datasets-and-transformers-libraries\"><button class=\"gtn-boxify-button details\" type=\"button\" aria-controls=\"details-loaded-functions-and-classes-from-datasets-and-transformers-libraries\" aria-expanded=\"true\"><i class=\"fas fa-info-circle\" aria-hidden=\"true\" ></i> <span>Details: Loaded functions and classes from datasets and transformers libraries</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">datasets</code>:\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">load_dataset</code>: function to load datasets from the Hugging Face Hub or local files.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code style=\"color: inherit\">transformers</code>:\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">AutoConfig</code>: Automatically loads the configuration for a pre-trained model. It defines the architecture and hyperparameters of the model.</li>\n",
    "<li><code style=\"color: inherit\">AutoModelForCausalLM</code>: Loads a pre-trained causal language model for tasks like text generation, where the model predicts the next token in a sequence.</li>\n",
    "<li><code style=\"color: inherit\">AutoTokenizer</code>: Loads the tokenizer associated with a pre-trained model. It converts text into tokens that the model can process.</li>\n",
    "<li><code style=\"color: inherit\">DataCollatorForLanguageModeling</code>: A data collator specifically designed for language modeling tasks. It prepares batches of data for training by handling padding and masking.</li>\n",
    "<li><code style=\"color: inherit\">EarlyStoppingCallback</code>: A callback used during training to stop the process early if the models performance on the validation set stops &gt; improving, saving time and resources.</li>\n",
    "<li><code style=\"color: inherit\">Trainer</code>: A high-level API for training and evaluating transformer &gt; models. It simplifies the training loop and handles tasks like gradient accumulation and evaluation.</li>\n",
    "<li><code style=\"color: inherit\">TrainingArguments</code>: A class to define the training configuration, including hyperparameters like learning rate, batch size, and number &gt; of epochs. It is used to configure the <code style=\"color: inherit\">Trainer</code>.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p>These components work together to streamline the process of training and fine-tuning transformer models for various NLP tasks.</p>\n",
    "</blockquote>\n",
    "<blockquote class=\"comment\" style=\"border: 2px solid #ffecc1; margin: 1em 0.2em\">\n",
    "<div class=\"box-title comment-title\" id=\"comment-versions\"><i class=\"far fa-comment-dots\" aria-hidden=\"true\" ></i> Comment: Versions</div>\n",
    "<p>This tutorial has been tested with following versions:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">accelerate</code> &gt; 0.32.1</li>\n",
    "<li><code style=\"color: inherit\">flash_attn</code> &gt; 2.6.0.post1 and 2.7.0.post2</li>\n",
    "<li><code style=\"color: inherit\">transformers</code> &gt; 4.47.1</li>\n",
    "</ul>\n",
    "<p>You can check the versions with:</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div><pre style=\"color: inherit; background: transparent\"><code style=\"color: inherit\">accelerate.__version__\n",
    "flash_attn.__version__\n",
    "transformers.__version__\n",
    "</code></pre></div>  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad97ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from flash-attn) (2.5.1)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (2025.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp310-cp310-linux_x86_64.whl size=255955166 sha256=c59be18fa934e132e5a405bcca6673af1d9d09a0036a9e081133bc7b7fc2992e\n",
      "  Stored in directory: /mnt/data/.cache/pip_cache/wheels/f5/05/1e/a6726e9eee2e7ee6151dbfed113e89d220dd3964ba617ab32d\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn]2\u001b[0m [flash-attn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "!pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa5dac0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'torch_gpu_dna (Python 3.10.18)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n torch_gpu_dna ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import accelerate\n",
    "# import flash_attn\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoConfig, # load the configuration of pre-trained model. architecture and hyperparameter of the model\n",
    "    AutoModelForCausalLM, # loads the pretrained causal language model for task like text generation\n",
    "    AutoTokenizer, # load the tokenizer with a pre-trained model. convert the text to tokens\n",
    "    DataCollatorForLanguageModeling, # designed for language modelling task. prepares batches for training by handling padding and masking\n",
    "    EarlyStoppingCallback,  # is used to stop the training, if in the validation performance stops improving to save time and resources\n",
    "    Trainer, # A high level API for training and evaluating the transformers. \n",
    "    TrainingArguments, # define the hyperparameter like learning rate, batch size, epoch, weight decay. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d83aa0",
   "metadata": {},
   "source": [
    "# Choose the LLM architecture\n",
    "\n",
    "Lets look at the original archicture of Mixtral-8x7B-v0.1 which is stored in the data/models/Mixtral-8x7B-v0.1 folder Github https://github.com/raphaelmourad/Mistral-DNA/tree/main/data/models/Mixtral-8x7B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad6d2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "savedir = \"/mnt/data/projects/.immune/Personal/DNA-Language-Model/Mistral_DNA/\"\n",
    "os.chdir(savedir)\n",
    "config = AutoConfig.from_pretrained(\"data/models/Mixtral-8x7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86f60f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralConfig {\n",
       "  \"_name_or_path\": \"data/models/Mixtral-8x7B-v0.1\",\n",
       "  \"architectures\": [\n",
       "    \"MixtralForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 256,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"mixtral\",\n",
       "  \"num_attention_heads\": 8,\n",
       "  \"num_experts_per_tok\": 1,\n",
       "  \"num_hidden_layers\": 8,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"num_local_experts\": 64,\n",
       "  \"output_router_logits\": false,\n",
       "  \"rms_norm_eps\": 1e-05,\n",
       "  \"rope_theta\": 1000000.0,\n",
       "  \"router_aux_loss_coef\": 0.02,\n",
       "  \"sliding_window\": null,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.38.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 4096\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f677461",
   "metadata": {},
   "source": [
    "By loading the configuration, we can inspect or modify the models architecture without loading the actual model weights. Lets now initialize a causal language model from the loaded configuration object, with a specific attention implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a17cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Dec 20 05:42:19 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       On  |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   34C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772d31f",
   "metadata": {},
   "source": [
    "<p>By loading the configuration, we can inspect or modify the models architecture without loading the actual model weights. Lets now initialize a causal language model from the loaded configuration object, with a specific attention implementation:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a7f7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_config(config, attn_implementation=\"eager\")\n",
    "# eager specifies the attention implementatin to use. Attention mechanism will be executed \n",
    "# eagerly which can be useful for debugging or when working with dynamic computation graphs\n",
    "# Eager execution runs operations immediatedy as they are called in Python rather than adding \n",
    "# them to graph execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0cfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is GPT2 model architecture\n",
    "# DNAGPT(\n",
    "#   (transformers): ModuleDict(\n",
    "#     (wte): Embedding(4348, 512)\n",
    "#     (wpe): Embedding(1024, 512)\n",
    "#     (h): ModuleList(\n",
    "#       (0-15): 16 x Block(\n",
    "#         (ln_f1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "#         (self_attn): CausalSelfAttention(\n",
    "#           (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n",
    "#           (c_proj): Linear(in_features=512, out_features=512, bias=True)\n",
    "#         )\n",
    "#         (ln_f2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "#         (mlp): MLP(\n",
    "#           (ln1): Linear(in_features=512, out_features=2048, bias=True)\n",
    "#           (nln): GELU(approximate='tanh')\n",
    "#           (ln2): Linear(in_features=2048, out_features=512, bias=True)\n",
    "#         )\n",
    "#       )\n",
    "#     )\n",
    "#     (ln_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=512, out_features=4348, bias=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed5e211c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForCausalLM(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(4096, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-63): 64 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w2): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (w3): Linear(in_features=256, out_features=256, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=4096, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aa00d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameter 105.0 million\n"
     ]
    }
   ],
   "source": [
    "Total_parameters = sum(p.numel() for p in model.parameters()) / 1000 ** 2\n",
    "print(f\"Total Parameter {Total_parameters:.1f} million\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387537fe",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "##### Embedding layer\n",
    "4096 input i.e. 4**6 4 = [A,T,G,C], 6 mers and 256 dimensions\n",
    "##### 8 Decoder layer\n",
    "query, key, value, output, rotary: Position information\n",
    "<p> This allows the model to weigh the importance of differenttokens in the sequence relative to each other, capturing dependenciesand context. </p>\n",
    "\n",
    "### MixtralSparseMoeBlock \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0795419b",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ffface2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data/tools/miniconda3/envs/torch_gpu_dna/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:186: UserWarning: The `resume_download` argument is deprecated and ignored in `hf_hub_download`. Downloads always resume whenever possible.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1fd67b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='zhihan1996/DNABERT-2-117M', vocab_size=4096, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7156b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 2061,  754,    6,    0,    0,  443,  156,    0,    0,    0,    5,\n",
      "            0,    0,    0,    0,    2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "['[CLS]', 'ATT', 'GCATTA', 'C', '[UNK]', '[UNK]', 'CCGG', 'GCCAA', '[UNK]', '[UNK]', '[UNK]', 'A', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    # \"ATGGCCTTAACCCCCCTCTGCGAATTACCATTGGGAGTTTCACCC\",\n",
    "    \"ATTGCATTACHHCCGGGCCAAKKKA!!##\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78d2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna = \"ACGTAGCATCGGATCTATCTATCGACACTTGGTTATCGATCTACGAGCATCTCGTTAGC\"\n",
    "inputs = tokenizer(dna, return_tensors = 'pt')[\"input_ids\"]\n",
    "# hidden_states = model(inputs)[0] # [1, sequence_length, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41ce95bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'A',\n",
       " 'CGTA',\n",
       " 'GCA',\n",
       " 'TCGGA',\n",
       " 'TCTATCTA',\n",
       " 'TCGACA',\n",
       " 'CTTGG',\n",
       " 'TTA',\n",
       " 'TCGA',\n",
       " 'TCTA',\n",
       " 'CGA',\n",
       " 'GCA',\n",
       " 'TCTC',\n",
       " 'GTTA',\n",
       " 'GC',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1273573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side  = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bbd3ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   1, 2061,    2]]), 'token_type_ids': tensor([[0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"ATT\", padding=\"longest\", return_tensors=\"pt\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c97ad6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "PreTrainedTokenizerFast is a fast and efficient tokenizer used to process text data for the DNABERT-2-117M model. \n",
    "\n",
    "Heres a breakdown of its configuration:<li>name_or_path='zhihan1996/DNABERT-2-117M': Specifies the name or path of the pre-trained tokenizer, indicating that it is associated with the DNABERT-2-117M model, which is designed for processing DNA sequences.</li>\n",
    "<li>vocab_size=4096 Defines the size of the tokenizers vocabulary. 4**6 (ATGC) * 6 mer = 4096</li>\n",
    "\n",
    "**Special_tokens:** Defines a set of special tokens used by the tokenizer: \n",
    "<li> unk_token: '[UNK]' - Represents unknown or out-of-vocabulary tokens.</li>\n",
    "<li> sep_token: '[SEP]' - Used to separate segments within a sequence. </li>\n",
    "<li> pad_token: '[PAD]' - Used for padding sequences to a uniform length. </li>\n",
    "<li> cls_token: '[CLS]' - Typically used as the first token in a sequence to represent the classification token.</li>\n",
    "<li> mask_token: '[MASK]' - Used in masked language modeling to hide tokens that the model must predict.</li>\n",
    "\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-11\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the other configuration parameters mean?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">model_max_length=1000000000000000019884624838656</code></li>\n",
    "<li><code style=\"color: inherit\">is_fast=True</code></li>\n",
    "<li><code style=\"color: inherit\">padding_side='right'</code></li>\n",
    "<li><code style=\"color: inherit\">truncation_side='right'</code></li>\n",
    "<li><code style=\"color: inherit\">clean_up_tokenization_spaces=False</code></li>\n",
    "<li><code style=\"color: inherit\">added_tokens_decoder</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary> View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-11\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-11\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">model_max_length=1000000000000000019884624838656</code>: Represents the maximum length of sequences that the model can handle.</p>\n",
    "<p>This extremely large value suggests that the model is designed to process very long sequences, although in practice, the actual limit will be constrained by available computational resources.</p>\n",
    "</li>\n",
    "<li><code style=\"color: inherit\">is_fast=True</code>: Indicates that this tokenizer is optimized for speed, leveraging Rust-based implementations to accelerate tokenization processes.</li>\n",
    "<li><code style=\"color: inherit\">padding_side='right'</code>: Configures the tokenizer to pad sequences on the right side, ensuring that all sequences in a batch have the same length by adding padding tokens to the end of shorter sequences.</li>\n",
    "<li><code style=\"color: inherit\">truncation_side='right'</code>: Specifies that sequences will be truncated from the right side if they exceed the maximum length, preserving the beginning of the sequence.</li>\n",
    "<li><code style=\"color: inherit\">clean_up_tokenization_spaces=False</code>: Indicates that the tokenizer will not remove spaces after tokenization, preserving the original spacing in the text.</li>\n",
    "<li><code style=\"color: inherit\">added_tokens_decoder</code>: Maps token IDs to their corresponding <code style=\"color: inherit\">AddedToken</code> objects, which include metadata such as whether the token is a special token and how it should be processed (e.g., stripping whitespace).</li>\n",
    "</ol>\n",
    "</blockquote>\n",
    "</blockquote>\n",
    "<p>This configuration ensures that the tokenizer is tailored to efficiently process DNA sequences, handling both the tokenization and padding/truncation of sequences in a manner that aligns with the models requirements.</p>\n",
    "<p>By default, tokenizers may pad sequences on the right side (<code class=\"language-plaintext highlighter-rouge\">padding_side='right'</code>). Lets set the padding direction for the tokenizer.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d662e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data based on BPE letter\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"longest\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2573da28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 99999 examples [00:00, 287295.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_text = load_dataset(\"csv\", data_files=\"data/genome_sequences/hg38/sequences_hg38_200b_verysmall.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "60e092c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/99999 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|| 99999/99999 [00:07<00:00, 13819.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset_text.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6655b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu_dna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
