{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92997ab4",
   "metadata": {},
   "source": [
    "# Enhancer Fine Tuning\n",
    "conda activate nt_finetune_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5622bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bba6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels_enhancers_types = 3\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\", num_labels=num_labels_enhancers_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e90947bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForSequenceClassification(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(4105, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1002, 1280, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=480, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (classifier): EsmClassificationHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1280, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a975871d",
   "metadata": {},
   "source": [
    "There are weights and bias that has been newly assigned. Since we have to perform training i.e. Dense classifier weights and classifier bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19856e7",
   "metadata": {},
   "source": [
    "## Change the model downstream classifier head\n",
    "Changes in the classifier head can be used for making different prediction like promotor classificaiton, regression etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094d428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmForSequenceClassification\n",
    "\n",
    "model2 = EsmForSequenceClassification.from_pretrained(\n",
    "    \"InstaDeepAI/nucleotide-transformer-500m-human-ref\",\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b74940c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForSequenceClassification(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(4105, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1002, 1280, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=480, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (classifier): EsmClassificationHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1280, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e783e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "raw_model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd50b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForSequenceClassification(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(4105, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1002, 1280, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=480, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (classifier): EsmClassificationHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1280, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f59dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "regression\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "raw_model.classifier = nn.Sequential(\n",
    "    nn.Linear(1280, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(512, 1)\n",
    ")\n",
    "\n",
    "raw_model.config.num_labels = 1\n",
    "raw_model.config.problem_type = \"regression\"\n",
    "print(raw_model.classifier)\n",
    "print(raw_model.config.problem_type)\n",
    "print(raw_model.config.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a92abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-500m-human-ref\")\n",
    "# AutoTokenizer.from_pretrained(...): This command downloads and loads the pre-trained tokenizer associated with the \n",
    "# specified model, \"InstaDeepAI/nucleotide-transformer-500m-human-ref\".\n",
    "# Purpose: The tokenizer converts raw DNA sequences (strings of 'A', 'C', 'G', 'T') into numerical input IDs and \n",
    "# attention masks that the transformer model can understand. For the Nucleotide Transformer, this typically involves \n",
    "# k-mer tokenization (breaking the sequence into overlapping or non-overlapping short nucleotide chunks). i.e. into 6 mers\n",
    "# So the sequence which is currently 300 bp converted into 50 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd19dfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(examples[\"data\"])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1aad4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset_name to None or 'default' since the prompt is about the config name\n",
    "# but 'promoter_all' is actually a value in the 'task' column.\n",
    "dataset_config_name = None \n",
    "\n",
    "# Load the full downstream tasks dataset\n",
    "full_train_dataset = load_dataset(\n",
    "    \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "    dataset_config_name,  # Pass None to use the default configuration\n",
    "    split=\"train\",\n",
    "    streaming=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2eaef45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sequence', 'name', 'label', 'task'],\n",
       "    num_rows: 461850\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef210dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H3K9ac', 'enhancers', 'H3K4me1', 'splice_sites_acceptors', 'promoter_all', 'enhancers_types', 'splice_sites_all', 'H3K4me3', 'H3K36me3', 'promoter_tata', 'promoter_no_tata', 'H3K4me2', 'H3K79me3', 'H4ac', 'H4', 'H3K14ac', 'splice_sites_donors', 'H3'}\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "total_task = set(full_train_dataset['task'])\n",
    "print(total_task)\n",
    "print(len(total_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a280f5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(full_train_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aaf144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for the specific task \"promoter_all\"\n",
    "# Filter the training split\n",
    "train_dataset_enhancers = full_train_dataset.filter(\n",
    "    lambda example: example[\"task\"] == \"promoter_all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8de00131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_dataset_enhancers['task'])\n",
    "set(train_dataset_enhancers['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d900e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full test split\n",
    "full_test_dataset = load_dataset(\n",
    "    \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
    "    None,  # Use 'None' for the default configuration\n",
    "    split=\"test\",\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "14fe9e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'H3K9ac', 'enhancers', 'H3K4me1', 'splice_sites_acceptors', 'promoter_all', 'enhancers_types', 'splice_sites_all', 'H3K4me3', 'H3K36me3', 'promoter_tata', 'promoter_no_tata', 'H3K4me2', 'H3K79me3', 'H4ac', 'H4', 'H3K14ac', 'splice_sites_donors', 'H3'}\n",
      "{0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(set(full_test_dataset['task']))\n",
    "print(set(full_test_dataset['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41bda23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the test split\n",
    "test_dataset_promoter = full_test_dataset.filter(\n",
    "    lambda example: example[\"task\"] == \"promoter_all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f36a85aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'promoter_all'}\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "print(set(test_dataset_promoter['task']))\n",
    "print(set(test_dataset_promoter['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0013172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TTTCGTAAAACAACCATGTTGTCTTCTGCACAGTACTGTATTCAGTGGCGAAGATGGATAGCCAATCCTTAAGCTCGCTAAAGGTGTAAACCAGCAAAGGGGGAGCCCGCCTGCGATACAATTTGAGCCCTTGTCGCTGATTTGCTAAGAGAGGGAGGCAGATTGAGAGAGAGAGAGAGATAGAGGACCGAGAGTTAGGGATTGACTTTGGCGTGAGCGCGCGCATGTTGGAACAAGATCGGTGCTTATGGAAAGAGAGAGGGACCAGGACTACTATCCGACTGAAGAAGAAGAAGCCAG', 'CAAATTGGATTTTTCTTTTTCTTTCCTTCCTCCTTCCTTCCTTCTTTTTTTCTTTTAATTGGCAACTCAGATTTTTCGAAGTGTTTTTGCTATCTCACTGCTGGAAAGCCTGGTTCTGCCTTTCCTAAAATCTCGTGTGCAGGTTCGCACTCCGGCTACTTTCAGGCCTCTAGGGAGCCCAGGTAGCGGCGCGCACGCGCACGCGCACACTTCTCCCTCGCTGGTCTTCAGGCCCGGCCCGCCCTGTCCAGAGGCGCCGGGACCCAGGCGCCTGCAGCCGCCCGCCGGGCCGACGTCCCA']\n",
      "300\n",
      "0\n",
      "TGCTCATAAAGGCCTTGGGCGTTTCTCTACCAAGCCCCAGAAGGCCCCCGTGTACGAAAAACCGACGGTGTCAATGAGGCGCGGGCTCCCCGCTATCAAAAAACTAGCGGGGCTGAGTTGATGTCAGTTACGCCTTACAAGTTCCGGGAAAGGACCACCGTATGACTGAGAAGAAGACGTTCAGGCATAGCGCGTTGATGTACGGGCCATACGTATACTTACATTGATTGCCATTCAGTGAGGCGGAGCAGAGTCTGCGGCAACAGCAGTAGCGGGCCGCCGCCGCCGCCATGAACCCCG\n",
      "300\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get training data\n",
    "train_sequences_enhancers = train_dataset_enhancers['sequence']\n",
    "train_labels_enhancers = train_dataset_enhancers['label']\n",
    "\n",
    "# Split the dataset into a training and a validation dataset\n",
    "train_sequences_enhancers, validation_sequences_enhancers, train_labels_enhancers, validation_labels_enhancers = train_test_split(train_sequences_enhancers,\n",
    "                                                                              train_labels_enhancers, test_size=0.10, random_state=42)\n",
    "\n",
    "print(train_sequences_enhancers[0:2])\n",
    "print(len(train_sequences_enhancers[0]))\n",
    "print(train_labels_enhancers[0])\n",
    "print(validation_sequences_enhancers[0])\n",
    "print(len(validation_sequences_enhancers[0]))\n",
    "print(validation_labels_enhancers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1960dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "test_sequences_enhancers = test_dataset_promoter['sequence']\n",
    "test_labels_enhancers = test_dataset_promoter['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b897dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCGTTGAGATAGAATAGTCGAGCGACCGTAGCAGTTGTACTACATTTAACATTGGGTTTTACCTAGGCGCTTCCTCAGCTACAGCGGCTAAAGCAGTTACTCAGCTAGATGGGTTGCTAAGTAAGCTCTTCAGATCAGAGCATGACGAAACGACTTGACGATGCGGCAGTGGGAGCTGGGCCACGTGGAGACTACTACATCTATGAGGATAAGGTTCCGGCCCGGTCGACTGCAACAGCGCCACCCCCAGAACCTCCTAAGCTGGTCAACGATAAGCCCCACAATTTCGTCTCTGGAAT\n",
      "300\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_sequences_enhancers[0])\n",
    "print(len(test_sequences_enhancers[0]))\n",
    "print(test_labels_enhancers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffdf75d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the datasets\n",
    "# Enhancer dataset\n",
    "ds_train_enhancers = Dataset.from_dict({\"data\": train_sequences_enhancers,'labels':train_labels_enhancers})\n",
    "ds_validation_enhancers = Dataset.from_dict({\"data\": validation_sequences_enhancers,'labels':validation_labels_enhancers})\n",
    "ds_test_enhancers = Dataset.from_dict({\"data\": test_sequences_enhancers,'labels':test_labels_enhancers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd244e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      " Dataset({\n",
      "    features: ['data', 'labels'],\n",
      "    num_rows: 47948\n",
      "})\n",
      "Validation\n",
      " Dataset({\n",
      "    features: ['data', 'labels'],\n",
      "    num_rows: 5328\n",
      "})\n",
      "Test\n",
      " Dataset({\n",
      "    features: ['data', 'labels'],\n",
      "    num_rows: 5920\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\\n\",ds_train_enhancers)\n",
    "print(\"Validation\\n\",ds_validation_enhancers)\n",
    "print(\"Test\\n\",ds_test_enhancers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9fa62bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmTokenizer(name_or_path='InstaDeepAI/nucleotide-transformer-500m-human-ref', vocab_size=4107, model_max_length=1000, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3a141113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 47948/47948 [00:15<00:00, 3026.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Creating tokenized enhancer dataset\n",
    "tokenized_datasets_train_enhancers = ds_train_enhancers.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a5786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column([[3, 1393, 12, 651, 1501, 1660, 2262, 1865, 2275, 2832, 1992, 3717, 2644, 3698, 1043, 1860, 2620, 259, 3310, 3746, 3148, 352, 3753, 3517, 3163, 2320, 3328, 3984, 1488, 3280, 3280, 1235, 695, 855, 3867, 603, 3808, 3826, 3617, 1990, 202, 3965, 1152, 208, 3328, 2624, 2344, 1718, 1808, 784, 935], [3, 2057, 3865, 1433, 1433, 2654, 1690, 2413, 1625, 1385, 1289, 3974, 1589, 1375, 225, 1378, 1130, 637, 3847, 2689, 1662, 1389, 10, 1761, 3649, 1766, 1730, 1177, 2302, 1619, 3310, 983, 3058, 3634, 3634, 3622, 1438, 2493, 3945, 2302, 3054, 3755, 1680, 4030, 4046, 2303, 2684, 3774, 2995, 3766, 3500], [3, 2693, 2429, 1442, 3857, 2750, 2715, 1552, 3236, 564, 3250, 1971, 1646, 2999, 2879, 2322, 3386, 2765, 2766, 1344, 1965, 1710, 170, 1578, 1683, 2962, 1983, 3694, 1665, 3901, 3839, 3973, 3012, 1789, 603, 3745, 2474, 2684, 3826, 3675, 894, 3050, 4010, 2974, 3007, 3459, 890, 2290, 2671, 2841, 4074], [3, 3566, 3759, 3782, 4087, 3918, 2495, 3823, 3536, 3305, 4036, 1390, 1545, 3193, 2041, 637, 346, 839, 3138, 3311, 3950, 2503, 2981, 3906, 1783, 4098, 3895, 3059, 4007, 274, 2877, 2374, 2175, 3730, 2814, 4061, 3885, 977, 2234, 2794, 1506, 1581, 3482, 1634, 1791, 2973, 324, 88, 2651, 2436, 1328], [3, 2465, 2418, 3988, 240, 3812, 228, 3557, 1415, 3687, 2445, 2481, 1620, 1262, 2044, 1918, 2886, 2316, 1797, 1393, 2656, 11, 2948, 2967, 3907, 2042, 3117, 3323, 2496, 3257, 2647, 1518, 874, 988, 3263, 2267, 3756, 2384, 3945, 2813, 1410, 2755, 1660, 566, 3254, 3557, 1536, 2750, 1638, 1645, 3302]])\n",
      "47948\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets_train_enhancers['input_ids'])\n",
    "print(len(tokenized_datasets_train_enhancers['input_ids']))\n",
    "print(len(tokenized_datasets_train_enhancers['input_ids'][0])) # so by tokenization from 300 it is converted to 50.\n",
    "# 6 mers 300 / 6 = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6ef813e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "### First index is alway the beginning of the tokens\n",
    "print(tokenized_datasets_train_enhancers['input_ids'][0][0])\n",
    "print(tokenized_datasets_train_enhancers['input_ids'][1][0])\n",
    "print(tokenized_datasets_train_enhancers['input_ids'][2][0])\n",
    "print(tokenized_datasets_train_enhancers['input_ids'][3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a909c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5328/5328 [00:01<00:00, 2899.22 examples/s]\n",
      "Map: 100%|██████████| 5920/5920 [00:02<00:00, 2927.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets_validation_enhancers = ds_validation_enhancers.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")\n",
    "tokenized_datasets_test_enhancers = ds_test_enhancers.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bfe48e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets_test_enhancers['input_ids'][0][0])\n",
    "print(tokenized_datasets_test_enhancers['input_ids'][1][0])\n",
    "print(tokenized_datasets_validation_enhancers['input_ids'][0][0])\n",
    "print(tokenized_datasets_validation_enhancers['input_ids'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c0f18a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning \n",
    "batch_size = 8\n",
    "model_name='nucleotide-transformer_enhancer'\n",
    "args_enhancers = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-NucleotideTransformer\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps= 1,\n",
    "    per_device_eval_batch_size= 64,\n",
    "    num_train_epochs= 2,\n",
    "    logging_steps= 100,\n",
    "    load_best_model_at_end=True,  # Keep the best model according to the evaluation\n",
    "    metric_for_best_model=\"mcc_score\", # The mcc_score on the evaluation dataset used to select the best model\n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_drop_last=True,\n",
    "    max_steps= 1000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7a7f5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric for the evaluation\n",
    "def compute_metrics_mcc(eval_pred):\n",
    "    \"\"\"Computes Matthews correlation coefficient (MCC score) for binary classification\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    references = eval_pred.label_ids\n",
    "    r={'mcc_score': matthews_corrcoef(references, predictions)}\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ab3616b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_19807/61391906.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args_enhancers,\n",
    "    train_dataset= tokenized_datasets_train_enhancers,\n",
    "    eval_dataset= tokenized_datasets_validation_enhancers,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_mcc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "52a3efd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 21:12, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mcc Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.449981</td>\n",
       "      <td>0.797160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.368300</td>\n",
       "      <td>0.514276</td>\n",
       "      <td>0.764191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>0.334206</td>\n",
       "      <td>0.816509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.297700</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.818478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.275525</td>\n",
       "      <td>0.855552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.205800</td>\n",
       "      <td>0.265742</td>\n",
       "      <td>0.865453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.230200</td>\n",
       "      <td>0.267372</td>\n",
       "      <td>0.860214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.245200</td>\n",
       "      <td>0.232194</td>\n",
       "      <td>0.871608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.219733</td>\n",
       "      <td>0.871457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.197400</td>\n",
       "      <td>0.209710</td>\n",
       "      <td>0.875064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730e968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nt_finetune_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
